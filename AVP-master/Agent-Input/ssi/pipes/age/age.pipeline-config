live = true 										# $(bool) use live input from a microphone
file = media/emotion.wav 							# if not live input read in from this file

# classification

config = config/IS13_ComParE.conf					# name of openSMILE config file
													# path to models
model:root = 	../../models/speech/
model:arousal = arousal[IS13-scale-linear-c1e-3]	
model:valence = valence[IS13-scale-linear-c1e-3]
model:interest = interest[IS13-scale-linear-c1e-5]
model:gender = chunks.audio.compare.gender.gold.linsvm 
model:age = chunks.audio.compare.age.gold.linsvm	 

# vad detection

vad:scheme = filler
vad:annotator = gold
vad:modality = close
vad:feature = mfccdd
vad:feature_frame = 0.01
vad:feature_delta = 0.015
vad:feature_win = 0.04
vad:feature_context = 5
vad:feature_context_2 = 10
vad:model = lin
vad:model_params = -s 0 -e 0.01 -B 0.1

# active mq

activemq:use = false								# share results through activemq
activemq:uri = failover:(tcp://localhost:61616)		# activemq uri
activemq:id = SSI									# activemq id
activemq:topic= EMOTION								# activemq topic

# capture

capture:use = false									# capture user interaction
capture:dir = capture								# folder where media is stored
capture:user = true									# capture camera and audio
capture:screen = true								# capture screen

# websocket
websocket:use = false
websocket:port = 80
websocket:site = website